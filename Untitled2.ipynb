{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCxLel8-LWyO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Data generation\n",
        "# ---------------------------\n",
        "\n",
        "def generate_multivariate_series(n_series: int = 3,\n",
        "                                 n_steps: int = 2000,\n",
        "                                 seed: int = 42) -> pd.DataFrame:\n",
        "    \"\"\"Generate synthetic multivariate time series with at least two seasonalities.\n",
        "\n",
        "    Structure: value = trend + seasonal1 (long period) + seasonal2 (short period) + interaction + noise\n",
        "    We generate n_series parallel channels (features).\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    t = np.arange(n_steps)\n",
        "\n",
        "    data = np.zeros((n_steps, n_series))\n",
        "    for s in range(n_series):\n",
        "        trend = 0.01 * (t)  # linear trend\n",
        "        # Long seasonality (e.g., yearly-like) and short seasonality (weekly-like)\n",
        "        freq1 = 2 * np.pi / 200  # long period\n",
        "        freq2 = 2 * np.pi / 30   # short period\n",
        "        amp1 = 5 + s  # vary amplitude per series\n",
        "        amp2 = 2 + 0.5 * s\n",
        "        seasonal1 = amp1 * np.sin(freq1 * t + np.random.uniform(0, 2 * np.pi))\n",
        "        seasonal2 = amp2 * np.sin(freq2 * t + np.random.uniform(0, 2 * np.pi))\n",
        "        # multiplicative interaction\n",
        "        interaction = 0.1 * seasonal1 * seasonal2\n",
        "        noise = np.random.normal(scale=0.5 + 0.2 * s, size=n_steps)\n",
        "        data[:, s] = trend + seasonal1 + seasonal2 + interaction + noise\n",
        "\n",
        "    dates = pd.date_range(start='2000-01-01', periods=n_steps, freq='D')\n",
        "    df = pd.DataFrame(data, index=dates, columns=[f'feat_{i}' for i in range(n_series)])\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Dataset preparation\n",
        "# ---------------------------\n",
        "\n",
        "class SlidingWindowDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, input_len: int, output_len: int, scaler: StandardScaler = None):\n",
        "        self.df = df\n",
        "        self.input_len = input_len\n",
        "        self.output_len = output_len\n",
        "        self.n_features = df.shape[1]\n",
        "        self.scaler = scaler\n",
        "\n",
        "        values = df.values.astype(np.float32)\n",
        "        if scaler is not None:\n",
        "            values = scaler.fit_transform(values)\n",
        "\n",
        "        self.values = values\n",
        "        self.indices = []\n",
        "        L = len(values)\n",
        "        for i in range(L - input_len - output_len + 1):\n",
        "            self.indices.append(i)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i = self.indices[idx]\n",
        "        x = self.values[i: i + self.input_len]\n",
        "        y = self.values[i + self.input_len: i + self.input_len + self.output_len]\n",
        "        # return shapes: (input_len, n_features), (output_len, n_features)\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Transformer model (seq2seq)\n",
        "# ---------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_features: int,\n",
        "                 d_model: int = 64,\n",
        "                 nhead: int = 4,\n",
        "                 num_encoder_layers: int = 3,\n",
        "                 num_decoder_layers: int = 3,\n",
        "                 dim_feedforward: int = 256,\n",
        "                 dropout: float = 0.1,\n",
        "                 output_len: int = 24):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.d_model = d_model\n",
        "        self.output_len = output_len\n",
        "\n",
        "        # input projection to d_model\n",
        "        self.input_proj = nn.Linear(num_features, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(d_model=d_model,\n",
        "                                          nhead=nhead,\n",
        "                                          num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)\n",
        "\n",
        "        # project decoder output back to features\n",
        "        self.output_proj = nn.Linear(d_model, num_features)\n",
        "\n",
        "        # We will accumulate attention maps manually by re-implementing scaled-dot attention if needed.\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (batch, src_len, num_features)\n",
        "        # tgt: (batch, tgt_len, num_features)\n",
        "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        tgt = self.input_proj(tgt) * math.sqrt(self.d_model)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "\n",
        "        # create masks: transformer expects masks for tgt\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "\n",
        "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
        "        out = self.output_proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Training utilities\n",
        "# ---------------------------\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # teacher forcing: feed y_input as previous targets during training\n",
        "        y_input = torch.cat([x[:, -1:, :], y[:, :-1, :]], dim=1)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x, y_input)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            # autoregressive prediction during eval\n",
        "            batch_size = x.size(0)\n",
        "            y_pred = torch.zeros((batch_size, y.size(1), y.size(2)), device=device)\n",
        "            decoder_input = x[:, -1:, :]\n",
        "            for t in range(y.size(1)):\n",
        "                out = model(x, decoder_input)\n",
        "                next_step = out[:, -1:, :]\n",
        "                y_pred[:, t:t+1, :] = next_step\n",
        "                decoder_input = torch.cat([decoder_input, next_step], dim=1)\n",
        "            loss = criterion(y_pred, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            preds.append(y_pred.cpu().numpy())\n",
        "            trues.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    trues = np.concatenate(trues, axis=0)\n",
        "    return total_loss / len(dataloader.dataset), preds, trues\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Baseline LSTM model\n",
        "# ---------------------------\n",
        "\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, num_features, hidden_size=64, num_layers=2, output_len=24):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_features)\n",
        "        self.output_len = output_len\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (batch, src_len, num_features)\n",
        "        batch = src.size(0)\n",
        "        h, _ = self.lstm(src)\n",
        "        # last hidden step\n",
        "        last = h[:, -1, :]\n",
        "        # predict step-by-step autoregressively\n",
        "        outs = []\n",
        "        current = src[:, -1, :]\n",
        "        for _ in range(self.output_len):\n",
        "            # one-step through a linear layer from last\n",
        "            next_step = self.fc(last)\n",
        "            outs.append(next_step.unsqueeze(1))\n",
        "            # update last using a simple approach: feed next to LSTM (not stateful here)\n",
        "            # For simplicity, keep last fixed â€” this is a very simple baseline\n",
        "        out = torch.cat(outs, dim=1)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Metrics\n",
        "# ---------------------------\n",
        "\n",
        "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    # y_true, y_pred: (N, out_len, n_features)\n",
        "    # compute metrics aggregated across horizon and features\n",
        "    mae = mean_absolute_error(y_true.ravel(), y_pred.ravel())\n",
        "    # Removed squared=False for compatibility with older sklearn versions\n",
        "    rmse = np.sqrt(mean_squared_error(y_true.ravel(), y_pred.ravel()))\n",
        "    # MAPE (avoid divide by zero)\n",
        "    denom = np.where(np.abs(y_true) < 1e-8, 1e-8, np.abs(y_true))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Orchestration: training, evaluation, plotting\n",
        "# ---------------------------\n",
        "\n",
        "def run_experiment():\n",
        "    # Settings (you can change these)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Device:', device)\n",
        "\n",
        "    df = generate_multivariate_series(n_series=3, n_steps=1600)\n",
        "    # split train/val/test\n",
        "    train_df = df.iloc[:1200]\n",
        "    val_df = df.iloc[1200:1400]\n",
        "    test_df = df.iloc[1400:]\n",
        "\n",
        "    input_len = 60\n",
        "    output_len = 24\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_ds = SlidingWindowDataset(train_df, input_len, output_len, scaler=scaler)\n",
        "    val_ds = SlidingWindowDataset(val_df, input_len, output_len, scaler=scaler)\n",
        "    test_ds = SlidingWindowDataset(test_df, input_len, output_len, scaler=scaler)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model hyperparameters (small grid search)\n",
        "    models_info = []\n",
        "\n",
        "    # Transformer config\n",
        "    tf_config = {\n",
        "        'd_model': 64,\n",
        "        'nhead': 4,\n",
        "        'num_encoder_layers': 2,\n",
        "        'num_decoder_layers': 2,\n",
        "        'dim_feedforward': 128,\n",
        "        'dropout': 0.1\n",
        "    }\n",
        "\n",
        "    transformer = Seq2SeqTransformer(num_features=df.shape[1], output_len=output_len, **tf_config).to(device)\n",
        "    optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train for a few epochs\n",
        "    n_epochs = 8\n",
        "    best_val = float('inf')\n",
        "    best_model_state = None\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss = train_one_epoch(transformer, train_loader, optimizer, criterion, device)\n",
        "        val_loss, _, _ = evaluate_model(transformer, val_loader, criterion, device)\n",
        "        print(f'Epoch {epoch}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}')\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_model_state = transformer.state_dict()\n",
        "\n",
        "    transformer.load_state_dict(best_model_state)\n",
        "\n",
        "    # Evaluate on test\n",
        "    test_loss, preds, trues = evaluate_model(transformer, test_loader, criterion, device)\n",
        "    print('Transformer test loss (MSE):', test_loss)\n",
        "    # inverse transform\n",
        "    def inv(x):\n",
        "        # x shape (N, out_len, n_features)\n",
        "        N, L, F = x.shape\n",
        "        x2 = x.reshape(-1, F)\n",
        "        x_inv = scaler.inverse_transform(x2)\n",
        "        return x_inv.reshape(N, L, F)\n",
        "\n",
        "    preds_inv = inv(preds)\n",
        "    trues_inv = inv(trues)\n",
        "    tf_metrics = compute_metrics(trues_inv, preds_inv)\n",
        "    print('Transformer metrics:', tf_metrics)\n",
        "\n",
        "    # Baseline LSTM training (quick)\n",
        "    lstm = LSTMForecast(num_features=df.shape[1], hidden_size=64, num_layers=2, output_len=output_len).to(device)\n",
        "    optimizer_l = torch.optim.Adam(lstm.parameters(), lr=1e-3)\n",
        "    # training loop: simple\n",
        "    for epoch in range(1, 6):\n",
        "        lstm.train()\n",
        "        total = 0.0\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer_l.zero_grad()\n",
        "            out = lstm(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer_l.step()\n",
        "            total += loss.item() * x.size(0)\n",
        "        print(f'LSTM Epoch {epoch}: train_loss={total/len(train_loader.dataset):.6f}')\n",
        "\n",
        "    # Evaluate LSTM\n",
        "    lstm.eval()\n",
        "    preds_l = []\n",
        "    trues_l = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            out = lstm(x)\n",
        "            preds_l.append(out.cpu().numpy())\n",
        "            trues_l.append(y.cpu().numpy())\n",
        "    preds_l = np.concatenate(preds_l, axis=0)\n",
        "    trues_l = np.concatenate(trues_l, axis=0)\n",
        "    preds_l_inv = inv(preds_l)\n",
        "    trues_l_inv = inv(trues_l)\n",
        "    lstm_metrics = compute_metrics(trues_l_inv, preds_l_inv)\n",
        "    print('LSTM metrics:', lstm_metrics)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Visualization of forecasts vs actuals for a few examples\n",
        "    # ---------------------------\n",
        "    os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "    def plot_example(i: int, model_preds: np.ndarray, truths: np.ndarray, which: str):\n",
        "        # plot series for features\n",
        "        out_len = model_preds.shape[1]\n",
        "        fig, axes = plt.subplots(model_preds.shape[2], 1, figsize=(10, 3 * model_preds.shape[2]))\n",
        "        if model_preds.shape[2] == 1:\n",
        "            axes = [axes]\n",
        "        for f in range(model_preds.shape[2]):\n",
        "            ax = axes[f]\n",
        "            ax.plot(truths[i, :, f], label='true', marker='o')\n",
        "            ax.plot(model_preds[i, :, f], label='pred', marker='x')\n",
        "            ax.set_title(f'Example {i} - feature {f}')\n",
        "            ax.legend()\n",
        "        plt.tight_layout()\n",
        "        path = os.path.join('outputs', f'forecast_{which}_{i}.png')\n",
        "        fig.savefig(path)\n",
        "        plt.close(fig)\n",
        "\n",
        "    # plot a couple\n",
        "    for i in range(3):\n",
        "        plot_example(i, preds_inv, trues_inv, 'transformer')\n",
        "        plot_example(i, preds_l_inv, trues_l_inv, 'lstm')\n",
        "\n",
        "    # ---------------------------\n",
        "    # Attention weights inspection (approximation)\n",
        "    # ---------------------------\n",
        "    # Note: the nn.Transformer in PyTorch does not return attention weights by default.\n",
        "    # For a detailed per-head attention inspection you'd need to implement MultiheadAttention modules exposing weights.\n",
        "    # Here we'll do a small hack: we re-create a single MultiheadAttention layer to inspect attention for one example.\n",
        "\n",
        "    from torch.nn import MultiheadAttention\n",
        "\n",
        "    # take one batch sample\n",
        "    x_sample, y_sample = next(iter(test_loader))\n",
        "    x_sample = x_sample.to(device)\n",
        "    # project input\n",
        "    src_proj = transformer.input_proj(x_sample) * math.sqrt(transformer.d_model)\n",
        "    src_proj = transformer.pos_encoder(src_proj)\n",
        "\n",
        "    # use the transformer's encoder layers to inspect attention in the first encoder layer\n",
        "    # recreate multihead and compute attention manually for the first layer\n",
        "    mha = transformer.transformer.encoder.layers[0].self_attn\n",
        "    # MultiheadAttention returns attn_output, attn_output_weights (batch_size * num_heads, tgt_len, src_len)\n",
        "    # But calling it directly will return weights; we use it in batch_first=False mode, so transpose\n",
        "    q = k = v = src_proj.transpose(0, 1)  # (seq_len, batch, d_model)\n",
        "    attn_out, attn_weights = mha(q, k, v, need_weights=True)\n",
        "    # attn_weights shape: (batch_size * num_heads, tgt_len, src_len) or (batch, tgt_len, src_len) depending on torch version\n",
        "    attn_weights = attn_weights.detach().cpu().numpy()\n",
        "    # We'll summarize attention by averaging over heads\n",
        "    if attn_weights.ndim == 3:\n",
        "        # either (tgt_len, src_len, ) per head or (batch, tgt_len, src_len)\n",
        "        # if shape == (batch, tgt_len, src_len): average over tgt_len\n",
        "        attn_avg = attn_weights.mean(axis=0)\n",
        "    else:\n",
        "        attn_avg = attn_weights.mean(axis=0)\n",
        "\n",
        "    # Save a heatmap for attention for the first sample\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    cax = ax.imshow(attn_avg, aspect='auto')\n",
        "    ax.set_title('Average attention weights (encoder layer 0)')\n",
        "    ax.set_xlabel('key positions (src)')\n",
        "    ax.set_ylabel('query positions (tgt)')\n",
        "    fig.colorbar(cax)\n",
        "    fig.savefig(os.path.join('outputs', 'attention_encoder_layer0.png'))\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Provide a textual interpretation placeholder saved to file\n",
        "    with open(os.path.join('outputs', 'attention_interpretation.txt'), 'w') as f:\n",
        "        f.write('Attention interpretation:\\n')\n",
        "        f.write(' - Rows = query positions, Columns = key positions.\\n')\n",
        "        f.write(' - High values indicate which past time steps the encoder is focusing on when encoding a given position.\\n')\n",
        "        f.write(' - To interpret: look for banded patterns (seasonality) or diagonal dominance (local continuity).\\n')\n",
        "        f.write(' - Example insight (toy): if queries at positions corresponding to end-of-short-period consistently attend to positions offset by 30, model found the short-seasonality.\\n')\n",
        "\n",
        "    print('Outputs saved into outputs/ (plots, attention, interpretation)')\n",
        "    print('\\nSummary of metrics:')\n",
        "    print('Transformer:', tf_metrics)\n",
        "    print('LSTM:', lstm_metrics)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zagGoYtahje",
        "outputId": "3d9cc2c6-360f-424b-9d40-a42fbf3e0e6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Epoch 1: train_loss=0.252584, val_loss=0.252569\n",
            "Epoch 2: train_loss=0.106082, val_loss=0.203872\n",
            "Epoch 3: train_loss=0.076030, val_loss=0.236359\n",
            "Epoch 4: train_loss=0.060491, val_loss=0.180955\n",
            "Epoch 5: train_loss=0.052044, val_loss=0.203231\n",
            "Epoch 6: train_loss=0.047071, val_loss=0.187231\n",
            "Epoch 7: train_loss=0.043076, val_loss=0.166814\n",
            "Epoch 8: train_loss=0.040798, val_loss=0.153000\n",
            "Transformer test loss (MSE): 0.15036251835333994\n",
            "Transformer metrics: {'MAE': 1.4037426710128784, 'RMSE': np.float64(1.8414787268506474), 'MAPE': np.float32(9.936079)}\n",
            "LSTM Epoch 1: train_loss=0.883825\n",
            "LSTM Epoch 2: train_loss=0.445687\n",
            "LSTM Epoch 3: train_loss=0.227394\n",
            "LSTM Epoch 4: train_loss=0.190432\n",
            "LSTM Epoch 5: train_loss=0.176828\n",
            "LSTM metrics: {'MAE': 1.8042418956756592, 'RMSE': np.float64(2.3382587445798215), 'MAPE': np.float32(12.340375)}\n",
            "Outputs saved into outputs/ (plots, attention, interpretation)\n",
            "\n",
            "Summary of metrics:\n",
            "Transformer: {'MAE': 1.4037426710128784, 'RMSE': np.float64(1.8414787268506474), 'MAPE': np.float32(9.936079)}\n",
            "LSTM: {'MAE': 1.8042418956756592, 'RMSE': np.float64(2.3382587445798215), 'MAPE': np.float32(12.340375)}\n"
          ]
        }
      ]
    }
  ]
}